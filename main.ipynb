{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7472591,"sourceType":"datasetVersion","datasetId":4350392}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import the neccessary libraries which are required\nimport re\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import Counter\n\n# Load spaCy English model\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:40:02.916149Z","iopub.execute_input":"2024-02-04T09:40:02.916560Z","iopub.status.idle":"2024-02-04T09:40:03.624381Z","shell.execute_reply.started":"2024-02-04T09:40:02.916529Z","shell.execute_reply":"2024-02-04T09:40:03.623428Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\nqueries_df = pd.read_csv('/kaggle/input/dataset/queries.csv')\ndocs_df = pd.read_csv('/kaggle/input/dataset/docs.csv')\nqdrel_df = pd.read_csv('/kaggle/input/dataset/qdrel.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:40:03.625753Z","iopub.execute_input":"2024-02-04T09:40:03.626003Z","iopub.status.idle":"2024-02-04T09:40:03.648510Z","shell.execute_reply.started":"2024-02-04T09:40:03.625981Z","shell.execute_reply":"2024-02-04T09:40:03.647193Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# Task 1 ","metadata":{}},{"cell_type":"markdown","source":"*  Preprocess the docs and queries â€“ remove characters other than alphanumeric or whitespaces.\n*  Correct spelling in the queries and documents using SpaCy. Only for each query with some correction, print the original and corrected query in separate lines, followed by two newlines (\\n).\n*  Tokenize the words in the documents using spacy. Remove all words that occur in less than 5 documents or more than 85% of the documents.\n*  For each query, find the cosine similarity of its vector with that of the documents. Use this to find the top 5 and top 10 most similar documents.\n*  Calculate the Precision@k scores: report P@1, P@5 and P@10 averaged over all queries","metadata":{}},{"cell_type":"code","source":"# Preprocess the Docs and Queries\n# The purpose of preprocess_text function is to clean the text.\n# It removes alphanumeric or whitespaces in queries and docs.\n\ndef preprocess_text(text):\n    processed_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    return processed_text\n\nqueries_df['query_text_preprocessed'] = queries_df['query_text'].apply(preprocess_text)\ndocs_df['doc_text_preprocessed'] = docs_df['doc_text'].apply(preprocess_text)\n\n# print(queries_df)\n# print(docs_df)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:40:03.649671Z","iopub.execute_input":"2024-02-04T09:40:03.650006Z","iopub.status.idle":"2024-02-04T09:40:03.678784Z","shell.execute_reply.started":"2024-02-04T09:40:03.649979Z","shell.execute_reply":"2024-02-04T09:40:03.677293Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# the function correct_spelling corrects the spellings,\n# to get the correct version of the text.\n\ndef correct_spelling(text):\n    doc = nlp(text)\n    corrected_text = \" \".join(token.text for token in doc)    \n    return corrected_text\n\nqueries_df['query_text_corrected'] = queries_df['query_text_preprocessed'].apply(correct_spelling)\n\n# printing the original and corrected query\nfor i, row in queries_df.iterrows():\n    original_query = row['query_text_preprocessed']\n    corrected_query = row['query_text_corrected']\n    if(original_query != row['query_text_corrected']):\n        #print if corrected is not equal to original query\n        if (original_query != corrected_query):\n            print(f\"Original Query: {original_query}\")\n            print(f\"Corrected Query: {corrected_query}\\n\\n\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:40:03.681271Z","iopub.execute_input":"2024-02-04T09:40:03.681587Z","iopub.status.idle":"2024-02-04T09:40:04.323551Z","shell.execute_reply.started":"2024-02-04T09:40:03.681561Z","shell.execute_reply":"2024-02-04T09:40:04.322599Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Original Query: Kindly tell me whole process of admission at vits Vellore for biotechi m a bio student in 12I dont have math there\nCorrected Query: Kindly tell me whole process of admission at vits Vellore for biotechi m a bio student in 12I do nt have math there\n\n\nOriginal Query: Whats your secret to success\nCorrected Query: What s your secret to success\n\n\nOriginal Query: Why do people say Dhanush South Indian actor is ugly I dont think so\nCorrected Query: Why do people say Dhanush South Indian actor is ugly I do nt think so\n\n\nOriginal Query: How do I reset my Gmail password when I dont remember my recovery information\nCorrected Query: How do I reset my Gmail password when I do nt remember my recovery information\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# tokenize the text in the document\n\ntokenized_docs = []\n\nfor _, row in docs_df.iterrows():\n    doc_text = row['doc_text_preprocessed']\n    doc_tokens = []\n    for token in nlp(doc_text):\n        if token.is_alpha and not token.is_stop:\n            doc_tokens.append(token.text.lower())\n    tokenized_docs.append(doc_tokens)\nall_tokens = [token for doc_tokens in tokenized_docs for token in doc_tokens] \n    \n# Count frequency of each word\n# And remove all words that occur in less than 5\n# documents or more than 85% of the documents\n\nword_document_frequency = Counter(all_tokens)\nmin_document_frequency = 5\nmax_document_frequency_percentage = 0.85\n\nfiltered_tokens=[]\n#filtered_tokens \nfor token, count in word_document_frequency.items():\n    if min_document_frequency <= count <= len(docs_df) * max_document_frequency_percentage:\n        filtered_tokens.append(token)\n        \n# Tokenize documents & queries using the filtered vocabulary\n\ntokenized_docs = []\nfor doc_text in docs_df['doc_text_preprocessed']:\n    doc_tokens = [tok.text.lower() for tok in nlp(doc_text) if tok.text.lower() in filtered_tokens]\n    tokenized_docs.append(doc_tokens)\n\ntokenized_queries = []\nfor query_text in queries_df['query_text_corrected']:\n    query_tokens = [tok.text.lower() for tok in nlp(query_text) if tok.text.lower() in filtered_tokens]\n    tokenized_queries.append(query_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:40:04.326780Z","iopub.execute_input":"2024-02-04T09:40:04.327077Z","iopub.status.idle":"2024-02-04T09:42:09.302789Z","shell.execute_reply.started":"2024-02-04T09:40:04.327051Z","shell.execute_reply":"2024-02-04T09:42:09.301279Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Create TF-IDF \n# For each document and query the TF-IDF vectors\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_docs + tokenized_queries])\ntfidf_doc_matrix = tfidf_matrix[:len(docs_df)]\ntfidf_queries_matrix = tfidf_matrix[len(docs_df):]\n\nprint(\"\\nTF-IDF Matrix for Queries:\")\nprint(tfidf_queries_matrix)\n\nprint(\"\\nTF-IDF Matrix for Document:\")\nprint(tfidf_doc_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:42:09.304577Z","iopub.execute_input":"2024-02-04T09:42:09.304892Z","iopub.status.idle":"2024-02-04T09:42:09.376234Z","shell.execute_reply.started":"2024-02-04T09:42:09.304866Z","shell.execute_reply":"2024-02-04T09:42:09.375055Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"\nTF-IDF Matrix for Queries:\n  (0, 1409)\t0.6876081489380635\n  (0, 106)\t0.5372837027503974\n  (0, 1541)\t0.4883863801057443\n  (1, 170)\t1.0\n  (3, 650)\t1.0\n  (4, 2053)\t0.5337099720015381\n  (4, 1858)\t0.6274904124627819\n  (4, 1387)\t0.4105031208761348\n  (4, 958)\t0.3910199941235213\n  (5, 1105)\t0.5209225664041514\n  (5, 622)\t0.8536039361500816\n  (6, 553)\t0.7746235893302349\n  (6, 353)\t0.6324225603606687\n  (7, 1646)\t0.7225072874844494\n  (7, 1349)\t0.6913633050226656\n  (8, 1183)\t0.45957608669235206\n  (8, 27)\t0.46696082530765126\n  (8, 1316)\t0.2840229847626598\n  (8, 1494)\t0.42157269809062087\n  (8, 1837)\t0.3895450492149863\n  (8, 1900)\t0.4007486333747614\n  (9, 690)\t0.4810378733611324\n  (9, 266)\t0.4909296377731392\n  (9, 188)\t0.3906393868051933\n  (9, 1657)\t0.48746872945023484\n  :\t:\n  (91, 597)\t0.5865181077656336\n  (91, 337)\t0.5840967867765781\n  (91, 891)\t0.5610948698217595\n  (92, 1610)\t1.0\n  (93, 1749)\t0.8441007948952699\n  (93, 1084)\t0.5361845279912254\n  (94, 449)\t0.6179342718794169\n  (94, 9)\t0.5754689218439807\n  (94, 990)\t0.5357170481033635\n  (95, 1761)\t0.5845864215634858\n  (95, 2061)\t0.516994940646129\n  (95, 1474)\t0.5418141351886652\n  (95, 172)\t0.3121095800831219\n  (96, 321)\t0.6980959468702652\n  (96, 156)\t0.7160042241239278\n  (97, 1511)\t0.5580255944322103\n  (97, 250)\t0.5499520940063659\n  (97, 266)\t0.49873388793997836\n  (97, 1968)\t0.37070829404919514\n  (98, 1867)\t0.8901489670148992\n  (98, 172)\t0.4556696352866932\n  (99, 617)\t0.5549417283345509\n  (99, 1675)\t0.5348556244471196\n  (99, 377)\t0.5115993750775785\n  (99, 814)\t0.37978312043925083\n\nTF-IDF Matrix for Document:\n  (0, 958)\t0.2086962900342381\n  (0, 1170)\t0.33144983393845984\n  (0, 1715)\t0.34706076663139107\n  (0, 1001)\t0.35190130235292144\n  (0, 1818)\t0.7760806425724465\n  (1, 1170)\t0.33891251810564216\n  (1, 1715)\t0.3548749352415076\n  (1, 1001)\t0.3598244569560629\n  (1, 1818)\t0.79355431167936\n  (2, 1828)\t1.0\n  (3, 818)\t0.6005004983481833\n  (3, 959)\t0.5240974495409103\n  (3, 862)\t0.6039213648053002\n  (4, 387)\t0.5849704688745183\n  (4, 990)\t0.46919850103479493\n  (4, 1791)\t0.5040144913760679\n  (4, 956)\t0.42852270611293325\n  (5, 854)\t0.5372449144681001\n  (5, 957)\t0.5460570405430042\n  (5, 990)\t0.43798646006048747\n  (5, 1791)\t0.4704864197352168\n  (6, 1767)\t0.660341864737036\n  (6, 1209)\t0.7509651268039109\n  (7, 721)\t1.0\n  (8, 263)\t0.5257176654531796\n  :\t:\n  (9995, 997)\t0.39456848931992305\n  (9995, 426)\t0.3316643905466288\n  (9995, 1036)\t0.32245581436791804\n  (9995, 1884)\t0.3241582886744678\n  (9995, 841)\t0.35837324080130284\n  (9995, 1298)\t0.24579462413422007\n  (9995, 1709)\t0.31476431469318417\n  (9995, 39)\t0.30347482331132974\n  (9996, 525)\t0.4330183815606356\n  (9996, 1624)\t0.4330183815606356\n  (9996, 812)\t0.324673289758713\n  (9996, 1884)\t0.3737251590649818\n  (9996, 841)\t0.413171901205342\n  (9996, 1035)\t0.29458545621403437\n  (9996, 39)\t0.3498789960855847\n  (9997, 586)\t0.542277058928879\n  (9997, 1)\t0.542277058928879\n  (9997, 678)\t0.4877640267088582\n  (9997, 39)\t0.4170820506418974\n  (9998, 586)\t0.5436049995928891\n  (9998, 1)\t0.5436049995928891\n  (9998, 181)\t0.5436049995928891\n  (9998, 2102)\t0.3368691337193795\n  (9999, 1596)\t0.7407591641636736\n  (9999, 1847)\t0.6717706905689883\n","output_type":"stream"}]},{"cell_type":"code","source":"# For each query, find the cosine similarity of its vector with that of the documents\ncosine_similarities = cosine_similarity(tfidf_queries_matrix, tfidf_doc_matrix)\nprint(f\"\\n For each query, the cosine similarity of its vector with that of the documents is :\\n\", cosine_similarities)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:42:09.377647Z","iopub.execute_input":"2024-02-04T09:42:09.377937Z","iopub.status.idle":"2024-02-04T09:42:09.390552Z","shell.execute_reply.started":"2024-02-04T09:42:09.377910Z","shell.execute_reply":"2024-02-04T09:42:09.389210Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"\n For each query, the cosine similarity of its vector with that of the documents is :\n [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# get_top_indices function to get the indices of the top N scores in descending order.\ndef get_top_indices(similarity_scores, N):\n    return np.argsort(similarity_scores)[::-1][:N]\n\ntop_5_indices = []\ntop_10_indices = []\n\nfor similarities in cosine_similarities:\n    top_5_indices.append(get_top_indices(similarities, 5))\n    top_10_indices.append(get_top_indices(similarities, 10))\n\n# print the results\nfor i, (top_5, top_10) in enumerate(zip(top_5_indices, top_10_indices)):\n    print(f\"\\n\\nTop 5 documents for Query {i + 1}: {top_5}\")\n    print(f\"Top 10 documents for Query {i + 1}: {top_10}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:42:09.392395Z","iopub.execute_input":"2024-02-04T09:42:09.392784Z","iopub.status.idle":"2024-02-04T09:42:09.439367Z","shell.execute_reply.started":"2024-02-04T09:42:09.392754Z","shell.execute_reply":"2024-02-04T09:42:09.438513Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"\n\nTop 5 documents for Query 1: [1328   44 4275 2521 1720]\nTop 10 documents for Query 1: [1328   44 4275 2521 1720 4439 8892 2287 4274 2520]\n\n\nTop 5 documents for Query 2: [7669  417 4907 3978 1850]\nTop 10 documents for Query 2: [7669  417 4907 3978 1850 2137 2136 2249 2250 6915]\n\n\nTop 5 documents for Query 3: [9999 3329 3336 3335 3334]\nTop 10 documents for Query 3: [9999 3329 3336 3335 3334 3333 3332 3331 3330 3328]\n\n\nTop 5 documents for Query 4: [9954 2138 9978 2854 7717]\nTop 10 documents for Query 4: [9954 2138 9978 2854 7717 7354 9979 2139 6134 6133]\n\n\nTop 5 documents for Query 5: [9221  249 8140 3288  561]\nTop 10 documents for Query 5: [9221  249 8140 3288  561  560 5288 6682 8660  269]\n\n\nTop 5 documents for Query 6: [7385 4947 4946  138 2103]\nTop 10 documents for Query 6: [7385 4947 4946  138 2103 9449  276 3087 5858 4090]\n\n\nTop 5 documents for Query 7: [7083 1052 8311 1843  516]\nTop 10 documents for Query 7: [7083 1052 8311 1843  516 7207 8310 9451 5466  691]\n\n\nTop 5 documents for Query 8: [4817 8519 8644 8668 2402]\nTop 10 documents for Query 8: [4817 8519 8644 8668 2402 3116 8520 5959 5960 8901]\n\n\nTop 5 documents for Query 9: [2532 6221 9251 5834 3576]\nTop 10 documents for Query 9: [2532 6221 9251 5834 3576 6435 3201 1058 8376 9252]\n\n\nTop 5 documents for Query 10: [8671 8148 2107  983 4283]\nTop 10 documents for Query 10: [8671 8148 2107  983 4283 7757 7618 9515 1580 4989]\n\n\nTop 5 documents for Query 11: [9999 3329 3336 3335 3334]\nTop 10 documents for Query 11: [9999 3329 3336 3335 3334 3333 3332 3331 3330 3328]\n\n\nTop 5 documents for Query 12: [2348 4497 8252 7279 1361]\nTop 10 documents for Query 12: [2348 4497 8252 7279 1361 1362 1235 1236 8106 6322]\n\n\nTop 5 documents for Query 13: [6028 9505 2609 1741   23]\nTop 10 documents for Query 13: [6028 9505 2609 1741   23 9222 1625 9506 6029 5681]\n\n\nTop 5 documents for Query 14: [9823 7265  312 7514 1266]\nTop 10 documents for Query 14: [9823 7265  312 7514 1266  803 8826 1267 8838 9113]\n\n\nTop 5 documents for Query 15: [5872 3388 9388 9387 2426]\nTop 10 documents for Query 15: [5872 3388 9388 9387 2426 2425 3183 9491 5814 3182]\n\n\nTop 5 documents for Query 16: [7604 3113 5869 5209 1641]\nTop 10 documents for Query 16: [7604 3113 5869 5209 1641 9793 5208 9068 8640 6882]\n\n\nTop 5 documents for Query 17: [9424 6893 6894 5092 3376]\nTop 10 documents for Query 17: [9424 6893 6894 5092 3376 2414 9175 3307 2732 3308]\n\n\nTop 5 documents for Query 18: [4721 1078 1077 3725 1683]\nTop 10 documents for Query 18: [4721 1078 1077 3725 1683 9999 3332 3337 3336 3335]\n\n\nTop 5 documents for Query 19: [8443 2730 6240 7244 7245]\nTop 10 documents for Query 19: [8443 2730 6240 7244 7245 3041 7693 7007 7008 6938]\n\n\nTop 5 documents for Query 20: [5599 7889 5929 5600 1988]\nTop 10 documents for Query 20: [5599 7889 5929 5600 1988 1989 7595 6033 6570 6882]\n\n\nTop 5 documents for Query 21: [9578 7205 4824 6834 1513]\nTop 10 documents for Query 21: [9578 7205 4824 6834 1513 4301 4013 4297 4296 6022]\n\n\nTop 5 documents for Query 22: [7587 2895 1722 2759 8302]\nTop 10 documents for Query 22: [7587 2895 1722 2759 8302 3476 5170 8140 5571 5419]\n\n\nTop 5 documents for Query 23: [3753 4871 6741 6740  115]\nTop 10 documents for Query 23: [3753 4871 6741 6740  115 9147  116 6685 9708 3289]\n\n\nTop 5 documents for Query 24: [5347 7748 7787 2709 1252]\nTop 10 documents for Query 24: [5347 7748 7787 2709 1252 1253 6656 6657 8640 6950]\n\n\nTop 5 documents for Query 25: [2698 7888 7887 8715 7402]\nTop 10 documents for Query 25: [2698 7888 7887 8715 7402 7401 9096 7927 5298 5012]\n\n\nTop 5 documents for Query 26: [5347 7748 7787 2709 1252]\nTop 10 documents for Query 26: [5347 7748 7787 2709 1252 1253 6656 6657 8640 6950]\n\n\nTop 5 documents for Query 27: [ 370 2257  965 2025 5876]\nTop 10 documents for Query 27: [ 370 2257  965 2025 5876 5844 6888 8192 1682 4091]\n\n\nTop 5 documents for Query 28: [8539 5379 6467 8358 5378]\nTop 10 documents for Query 28: [8539 5379 6467 8358 5378 2100 2101 6466 6114  830]\n\n\nTop 5 documents for Query 29: [7092  848 2797 3892 2588]\nTop 10 documents for Query 29: [7092  848 2797 3892 2588 6083 2428 6119 4230 2589]\n\n\nTop 5 documents for Query 30: [2668 3691 3692 8677 7343]\nTop 10 documents for Query 30: [2668 3691 3692 8677 7343 2199 9397 7957 5052  478]\n\n\nTop 5 documents for Query 31: [617 392 618 391  72]\nTop 10 documents for Query 31: [ 617  392  618  391   72 3332 3339 3338 3337 3336]\n\n\nTop 5 documents for Query 32: [8103 5462 4298 4299 7320]\nTop 10 documents for Query 32: [8103 5462 4298 4299 7320 7225 5980 5463 6286  383]\n\n\nTop 5 documents for Query 33: [5623 1999  353 1726 8411]\nTop 10 documents for Query 33: [5623 1999  353 1726 8411 8396 5054 2362 2361 3284]\n\n\nTop 5 documents for Query 34: [9999 3329 3336 3335 3334]\nTop 10 documents for Query 34: [9999 3329 3336 3335 3334 3333 3332 3331 3330 3328]\n\n\nTop 5 documents for Query 35: [8540 1433 2406 1434 4446]\nTop 10 documents for Query 35: [8540 1433 2406 1434 4446 9912 9913 4447 5839 5840]\n\n\nTop 5 documents for Query 36: [8103 5462 4298 4299 7320]\nTop 10 documents for Query 36: [8103 5462 4298 4299 7320 7225 5980 5463 6286  383]\n\n\nTop 5 documents for Query 37: [3678 3588 2159 2193 2194]\nTop 10 documents for Query 37: [3678 3588 2159 2193 2194 3587 8155 8156 2160 7586]\n\n\nTop 5 documents for Query 38: [9372 8211 5322 4566 5406]\nTop 10 documents for Query 38: [9372 8211 5322 4566 5406 2293 1652 1213 3145 2294]\n\n\nTop 5 documents for Query 39: [7459 2497 6613 9145 4769]\nTop 10 documents for Query 39: [7459 2497 6613 9145 4769 8771  404  403 2123 2050]\n\n\nTop 5 documents for Query 40: [2977  475 1976 8585 8547]\nTop 10 documents for Query 40: [2977  475 1976 8585 8547 7168 7169 8981  474 7042]\n\n\nTop 5 documents for Query 41: [2360 6628  749 4028 9652]\nTop 10 documents for Query 41: [2360 6628  749 4028 9652 5368 8559  982 2624 2941]\n\n\nTop 5 documents for Query 42: [9284 5844 5876 2025  965]\nTop 10 documents for Query 42: [9284 5844 5876 2025  965 4236 2160 8227 6159 6160]\n\n\nTop 5 documents for Query 43: [9737 7985 9312 9738 9045]\nTop 10 documents for Query 43: [9737 7985 9312 9738 9045 7668 7518 2975 2554 1114]\n\n\nTop 5 documents for Query 44: [2525 2524 9445  390 1717]\nTop 10 documents for Query 44: [2525 2524 9445  390 1717 2046 7519 8097 1782 3577]\n\n\nTop 5 documents for Query 45: [ 709 7152 2554 2975 3824]\nTop 10 documents for Query 45: [ 709 7152 2554 2975 3824 8099 4310 4843  796 8612]\n\n\nTop 5 documents for Query 46: [9999 3329 3336 3335 3334]\nTop 10 documents for Query 46: [9999 3329 3336 3335 3334 3333 3332 3331 3330 3328]\n\n\nTop 5 documents for Query 47: [ 982 8559 5368 6628 2360]\nTop 10 documents for Query 47: [ 982 8559 5368 6628 2360  749 4028 8141 7874 8638]\n\n\nTop 5 documents for Query 48: [3355  279 6749 4704 2006]\nTop 10 documents for Query 48: [3355  279 6749 4704 2006 2007 8053 4670 5979 6560]\n\n\nTop 5 documents for Query 49: [7023  528 8823 5562  393]\nTop 10 documents for Query 49: [7023  528 8823 5562  393 7024 1158 1357 1358 5440]\n\n\nTop 5 documents for Query 50: [1888 7811 4995 9913 9912]\nTop 10 documents for Query 50: [1888 7811 4995 9913 9912 5839 4447 4446 5840 8362]\n\n\nTop 5 documents for Query 51: [2921 5269 7478 6650 2310]\nTop 10 documents for Query 51: [2921 5269 7478 6650 2310 9142 6273 1622 6862 8507]\n\n\nTop 5 documents for Query 52: [6021 1334 2457 9220 9327]\nTop 10 documents for Query 52: [6021 1334 2457 9220 9327 2418 2245 2246 2575 2574]\n\n\nTop 5 documents for Query 53: [9836 5350 8768 9302 5453]\nTop 10 documents for Query 53: [9836 5350 8768 9302 5453 5351 9576 5183 1936 1937]\n\n\nTop 5 documents for Query 54: [7811 3433 1888 8362 2722]\nTop 10 documents for Query 54: [7811 3433 1888 8362 2722 6259 2505 6744 5272 5271]\n\n\nTop 5 documents for Query 55: [3220 7084 9178 9793 7225]\nTop 10 documents for Query 55: [3220 7084 9178 9793 7225 4298 5980 8103 7320 4299]\n\n\nTop 5 documents for Query 56: [1857 3018 1267 5180  803]\nTop 10 documents for Query 56: [1857 3018 1267 5180  803 8838 1266 9113 8826 8089]\n\n\nTop 5 documents for Query 57: [1011 6742 6725 4027 1273]\nTop 10 documents for Query 57: [1011 6742 6725 4027 1273 7377  881 1359 1010 2788]\n\n\nTop 5 documents for Query 58: [ 535  313  856 6742 6725]\nTop 10 documents for Query 58: [ 535  313  856 6742 6725 4027 1011 6887 4195 4194]\n\n\nTop 5 documents for Query 59: [6084 4497 2348 6299 4498]\nTop 10 documents for Query 59: [6084 4497 2348 6299 4498 7926 1201 6040 5741 6041]\n\n\nTop 5 documents for Query 60: [9617 9392  592 9391 2724]\nTop 10 documents for Query 60: [9617 9392  592 9391 2724 2723 5589 1325  692 9986]\n\n\nTop 5 documents for Query 61: [4050  651 5646 9899 8435]\nTop 10 documents for Query 61: [4050  651 5646 9899 8435 5647 8436 9898 8850 5049]\n\n\nTop 5 documents for Query 62: [9869  367 2893 2892 4954]\nTop 10 documents for Query 62: [9869  367 2893 2892 4954 9088 9087 8564 7530  366]\n\n\nTop 5 documents for Query 63: [2694  830 7918 9293 5379]\nTop 10 documents for Query 63: [2694  830 7918 9293 5379  397 7919  398 6066 1673]\n\n\nTop 5 documents for Query 64: [7969 7098 4839 4838 3414]\nTop 10 documents for Query 64: [7969 7098 4839 4838 3414 9481 2083 4641 4640 1857]\n\n\nTop 5 documents for Query 65: [9023  644 6839 4145 5467]\nTop 10 documents for Query 65: [9023  644 6839 4145 5467 1534 8589 5627 1533  891]\n\n\nTop 5 documents for Query 66: [ 409 3209 3208 5890 2414]\nTop 10 documents for Query 66: [ 409 3209 3208 5890 2414 3376 4925 5889 6500 8749]\n\n\nTop 5 documents for Query 67: [3835 1723  434  435 9913]\nTop 10 documents for Query 67: [3835 1723  434  435 9913 4446 4447 5839 9912 5840]\n\n\nTop 5 documents for Query 68: [2709 7748 7787 5347 5005]\nTop 10 documents for Query 68: [2709 7748 7787 5347 5005 7751 1252 7619 3916 3677]\n\n\nTop 5 documents for Query 69: [9100 7023  528 2070 7462]\nTop 10 documents for Query 69: [9100 7023  528 2070 7462 9401 8823 9022 5562 6641]\n\n\nTop 5 documents for Query 70: [2590 1589 7331 6374 7332]\nTop 10 documents for Query 70: [2590 1589 7331 6374 7332 6405 8232 4005 6682 4720]\n\n\nTop 5 documents for Query 71: [6748 6757 1361 8252 7279]\nTop 10 documents for Query 71: [6748 6757 1361 8252 7279 1362  760 1235 1236 3008]\n\n\nTop 5 documents for Query 72: [5002 2124 9155  104 2125]\nTop 10 documents for Query 72: [5002 2124 9155  104 2125 3921 1792  540  541 3060]\n\n\nTop 5 documents for Query 73: [7174 4850 8131  237 4851]\nTop 10 documents for Query 73: [7174 4850 8131  237 4851 1355 7070 1354 8132 4968]\n\n\nTop 5 documents for Query 74: [2736  896 1205 8084 8083]\nTop 10 documents for Query 74: [2736  896 1205 8084 8083 8010  897  312 6966 4539]\n\n\nTop 5 documents for Query 75: [ 852 7240 6860 6861  395]\nTop 10 documents for Query 75: [ 852 7240 6860 6861  395 5249 5195 5671  786 5194]\n\n\nTop 5 documents for Query 76: [9114 1403 5934 5581 5933]\nTop 10 documents for Query 76: [9114 1403 5934 5581 5933 3450 6408 7797 6566 6409]\n\n\nTop 5 documents for Query 77: [ 383  382 6286 7320 4298]\nTop 10 documents for Query 77: [ 383  382 6286 7320 4298 5980 7225 8103 5462 5463]\n\n\nTop 5 documents for Query 78: [1728 6347 3538 3176 4261]\nTop 10 documents for Query 78: [1728 6347 3538 3176 4261 4260 3177 1487 6034 3112]\n\n\nTop 5 documents for Query 79: [5345 8474 6802 6801 9874]\nTop 10 documents for Query 79: [5345 8474 6802 6801 9874 9875 3236 1590 4635 4634]\n\n\nTop 5 documents for Query 80: [1923 8388 4924 8795 7259]\nTop 10 documents for Query 80: [1923 8388 4924 8795 7259 6853 9748 5893 6430 5894]\n\n\nTop 5 documents for Query 81: [9084 4199  591 8700 4067]\nTop 10 documents for Query 81: [9084 4199  591 8700 4067 2495 7880 7879 4200 9888]\n\n\nTop 5 documents for Query 82: [8926 9799 3512 5598 1943]\nTop 10 documents for Query 82: [8926 9799 3512 5598 1943  419 9798 9544 5799 2560]\n\n\nTop 5 documents for Query 83: [7855 7856 1634 8753 2477]\nTop 10 documents for Query 83: [7855 7856 1634 8753 2477 6803 3483 1633 2889 3676]\n\n\nTop 5 documents for Query 84: [1398 4219 6705  347 3718]\nTop 10 documents for Query 84: [1398 4219 6705  347 3718  717 2634 6478  348 6208]\n\n\nTop 5 documents for Query 85: [4862 1424 5674 9739 5099]\nTop 10 documents for Query 85: [4862 1424 5674 9739 5099 4863 9369 3999 2837 4121]\n\n\nTop 5 documents for Query 86: [2576 8098 8964 2979 9581]\nTop 10 documents for Query 86: [2576 8098 8964 2979 9581 4633 6496 9826 9827 2741]\n\n\nTop 5 documents for Query 87: [8103 5462 4298 4299 7320]\nTop 10 documents for Query 87: [8103 5462 4298 4299 7320 7225 5980 5463 6286  383]\n\n\nTop 5 documents for Query 88: [ 245  244 9216 3214 7848]\nTop 10 documents for Query 88: [ 245  244 9216 3214 7848 6233 2500 9215 5973 2037]\n\n\nTop 5 documents for Query 89: [8352 9568 8353 6719 8051]\nTop 10 documents for Query 89: [8352 9568 8353 6719 8051 2292 2291 5341 4737 7794]\n\n\nTop 5 documents for Query 90: [7352 1600 1601 6054 1750]\nTop 10 documents for Query 90: [7352 1600 1601 6054 1750 1751 8056 8242 3081 5580]\n\n\nTop 5 documents for Query 91: [8220 6486 6487 7231 7827]\nTop 10 documents for Query 91: [8220 6486 6487 7231 7827  295 4240  860  861 1422]\n\n\nTop 5 documents for Query 92: [2694  830 6067 8604 7918]\nTop 10 documents for Query 92: [2694  830 6067 8604 7918 9293 5379  397 1280 1281]\n\n\nTop 5 documents for Query 93: [2678 9600  779 9601  855]\nTop 10 documents for Query 93: [2678 9600  779 9601  855 2331 3330 3337 3336 3335]\n\n\nTop 5 documents for Query 94: [1126 5974 2856 1501 5369]\nTop 10 documents for Query 94: [1126 5974 2856 1501 5369 1500 6937 4104  998 5624]\n\n\nTop 5 documents for Query 95: [6110 9767 6109 1332 1343]\nTop 10 documents for Query 95: [6110 9767 6109 1332 1343  257  256 7853 1933 2156]\n\n\nTop 5 documents for Query 96: [ 943 3267  748 6108 4228]\nTop 10 documents for Query 96: [ 943 3267  748 6108 4228 3342 8152 6978 8206 2941]\n\n\nTop 5 documents for Query 97: [ 371 1737 1738  882 1067]\nTop 10 documents for Query 97: [ 371 1737 1738  882 1067  988 2141 1066 4861 9980]\n\n\nTop 5 documents for Query 98: [2595 5203 2596 1125 2107]\nTop 10 documents for Query 98: [2595 5203 2596 1125 2107  458 6808 2760 3970 2046]\n\n\nTop 5 documents for Query 99: [2900 7958 4937 2040 8904]\nTop 10 documents for Query 99: [2900 7958 4937 2040 8904 3931 8028 2041 8905 1920]\n\n\nTop 5 documents for Query 100: [1046 5480 1355 1354 1048]\nTop 10 documents for Query 100: [1046 5480 1355 1354 1048 7684 9583 2606 1200 9582]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculates the Precision@k scores: report P@1, P@5 and P@10 averaged over all queries\n\nk_values = [1, 5, 10]\n# to store Precision@k scores for each k\nprecision_at_k = {k: [] for k in k_values}\n\n# precision@k for each query\nfor i,query_vector in enumerate(tfidf_queries_matrix):\n    query_id = queries_df['query_id'].iloc[i]\n    relevant_docs = set(qdrel_df[qdrel_df['query_id'] == query_id]['doc_id'])\n\n    # evaluate cosine similarities\n    cosine_similarities = cosine_similarity(query_vector, tfidf_doc_matrix)[0]\n    sorted_indices = np.argsort(cosine_similarities)[::-1]\n\n    # for each k calculating Precision@k\n    for k in k_values:\n        top_k_indices =sorted_indices[:k]\n        retrieved_docs = set(docs_df['doc_id'].iloc[top_k_indices])\n\n        # Calculating Precision@k\n        precision = 1 if (len(relevant_docs.intersection(retrieved_docs)) / k) >0  else 0\n        precision_at_k[k].append(precision)\n\n# Calculating average Precision@k\naverage_precision_at_k= {k: np.mean(scores) for k, scores in precision_at_k.items()}\n\n# # print the results\nprint(\"\\nAverage Precisions before Stemming and lemmatization : \")\nfor k, avg_precision in average_precision_at_k.items():\n    print(f\"\\nAverage Precision@{k}: {avg_precision}\")\n\n\nprint(\"\\nThe vocabulary size before Stemming and lemmatization : \",vectorizer.get_feature_names_out().shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:42:09.440715Z","iopub.execute_input":"2024-02-04T09:42:09.441030Z","iopub.status.idle":"2024-02-04T09:42:09.688410Z","shell.execute_reply.started":"2024-02-04T09:42:09.441005Z","shell.execute_reply":"2024-02-04T09:42:09.687211Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"\nAverage Precisions before Stemming and lemmatization : \n\nAverage Precision@1: 0.53\n\nAverage Precision@5: 0.76\n\nAverage Precision@10: 0.79\n\nThe vocabulary size before Stemming and lemmatization :  (2129,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 2","metadata":{}},{"cell_type":"markdown","source":"*  Improve the performance of Task1 by stemming the tokens (using spacy) before calculating the vocabulary.\n*  Improve the performance of Task1 by lemmatizing the tokens (using spacy) before calculating the vocabulary.\n*  Report the size of the vocabulary you obtained as part of Task 1, the vocabulary size after stemming and the vocabulary size after lemmatization.\n*  Report the performance metrics in both these cases and discuss the results (why or why not performance has increased).\n","metadata":{}},{"cell_type":"code","source":"# By stemming the tokens\nprint(\"\\n Improve the performance of Task1 by stemming the tokens \\n\")\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\n\ntokenized_docs = []\nall_tokens = []\n# using stemming to improve performance\nfor i, row in docs_df.iterrows():\n    doc_text = row['doc_text_preprocessed']\n    doc_tokens = [stemmer.stem(token.text.lower()) for token in nlp(doc_text) if not token.is_stop and token.is_alpha]\n    tokenized_docs.append(doc_tokens)\n    all_tokens.extend(doc_tokens)\n\n#countinf the frequency of tokens\nword_document_frequency = Counter(all_tokens)\n\n# Filter out tokens\nmin_document_frequency = 5\nmax_document_frequency_percentage = 0.85\n\nfiltered_tokens=[]\nfor token, count in word_document_frequency.items():\n    if min_document_frequency <= count <= len(docs_df) * max_document_frequency_percentage:\n        filtered_tokens.append(token)\n# Tokenize documents and queries using the filtered vocabulary\ntokenized_docs = []\n\nfor doc_text in docs_df['doc_text_preprocessed']:\n    doc_tokens = [tok.text.lower() for tok in nlp(doc_text) if tok.text.lower() in filtered_tokens]\n    tokenized_docs.append(doc_tokens)\n\ntokenized_queries = []\n\nfor query_text in queries_df['query_text_corrected']:\n    query_tokens = [tok.text.lower() for tok in nlp(query_text) if tok.text.lower() in filtered_tokens]\n    tokenized_queries.append(query_tokens)\n\n#print(tokenized_docs[0])\n# print(tokenized_queries)\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_docs + tokenized_queries])\n\n# Separate TF-IDF vecctors\ntfidf_doc_matrix = tfidf_matrix[:len(docs_df)]\ntfidf_queries_matrix = tfidf_matrix[len(docs_df):]\n\n# get the vocabulary size\nprint(\"After Stemming\")\nprint(\"The vocabulary size : \",vectorizer.get_feature_names_out().shape)\n\n# Calculate cosine similarity between query & documents\ncosine_similarities = cosine_similarity(tfidf_queries_matrix, tfidf_doc_matrix)\nprint(f\"\\n For each query, the cosine similarity of its vector with that of the documents is :\\n\", cosine_similarities)\n\n\n#to calculate performance matrix\n# Calculate Precision@k for each query\nfor i, query_vector in enumerate(tfidf_queries_matrix):\n    query_id = queries_df['query_id'].iloc[i]\n    relevant_docs = set(qdrel_df[qdrel_df['query_id'] == query_id]['doc_id'])\n\n    # cosine similarities\n    cosine_similarities = cosine_similarity(query_vector, tfidf_doc_matrix)[0]\n    sorted_indices = np.argsort(cosine_similarities)[::-1]\n\n    # Calculate Precision@k for each k\n    for k in k_values:\n        top_k_indices = sorted_indices[:k]\n        retrieved_docs = set(docs_df['doc_id'].iloc[top_k_indices])\n\n        # Calculating Precision@k\n        precision = 1 if (len(relevant_docs.intersection(retrieved_docs)) / k) >0  else 0\n        precision_at_k[k].append(precision)\n\n#average Precision@k\naverage_precision_at_k = {k: np.mean(scores) for k, scores in precision_at_k.items()}\n\n# # print the results\nprint(\"\\nAverage Precisions after stemming : \")\nfor k, avg_precision in average_precision_at_k.items():\n    print(f\"\\nAverage Precision@{k}: {avg_precision}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:42:09.691308Z","iopub.execute_input":"2024-02-04T09:42:09.691592Z","iopub.status.idle":"2024-02-04T09:44:14.663020Z","shell.execute_reply.started":"2024-02-04T09:42:09.691567Z","shell.execute_reply":"2024-02-04T09:44:14.661302Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"\n Improve the performance of Task1 by stemming the tokens \n\nAfter Stemming\nThe vocabulary size :  (1336,)\n\n For each query, the cosine similarity of its vector with that of the documents is :\n [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n\nAverage Precisions after stemming : \n\nAverage Precision@1: 0.43\n\nAverage Precision@5: 0.655\n\nAverage Precision@10: 0.705\n","output_type":"stream"}]},{"cell_type":"code","source":"# performance improvrment by lemmatization\nprint(\"\\n Improve the performance of Task1 by lemmatization the tokens \\n\")\n\ntokenized_docs = []\nall_tokens = []\n\n#lemmatizing the tokens\nfor i, row in docs_df.iterrows():\n    doc_text = row['doc_text_preprocessed']\n    doc_tokens = [token.lemma_ for token in nlp(doc_text) if not token.is_stop and token.is_alpha]\n    tokenized_docs.append(doc_tokens)\n    all_tokens.extend(doc_tokens)\n\n# fitlering out the vocubalary\nword_document_frequency = Counter(all_tokens)\nmin_document_frequency = 5\nmax_document_frequency_percentage = 0.85\n\nfiltered_tokens = [token for token, count in word_document_frequency.items() if min_document_frequency <= count <= len(docs_df) * max_document_frequency_percentage]\n\n# using filtered tokens, tokenize the doc and queries\ntokenized_docs = []\nfor doc_text in docs_df['doc_text_preprocessed']:\n    doc_tokens = [tok.text.lower() for tok in nlp(doc_text) if tok.text.lower() in filtered_tokens]\n    tokenized_docs.append(doc_tokens)\n\ntokenized_queries = []\nfor query_text in queries_df['query_text_corrected']:\n    query_tokens = [tok.text.lower() for tok in nlp(query_text) if tok.text.lower() in filtered_tokens]\n    tokenized_queries.append(query_tokens)\n\n# print(tokenized_docs[0])\n# print(tokenized_queries)\n# TF-IDF Vectorization\n\n#tf-idf vectors for both doc and query\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_docs + tokenized_queries])\n\n# Separate TF-IDF vectors for documents and queries\ntfidf_doc_matrix = tfidf_matrix[:len(docs_df)]\ntfidf_queries_matrix = tfidf_matrix[len(docs_df):]\n\nprint(\"After lemmatization : \\n\")\nprint(\"The vocabulary size : \",vectorizer.get_feature_names_out().shape)\n# print(tfidf_queries_matrix)\n\n# calculating cosine similarity of tfidf_queries_matrix & tfidf_doc_matrix\ncosine_similarities = cosine_similarity(tfidf_queries_matrix, tfidf_doc_matrix)\nprint(f\"\\n cosine similarity of tfidf_queries_matrix & tfidf_doc_matrix : \\n\", cosine_similarities)\n\n# Calculating the precisions\nprecision_at_k = {k: [] for k in k_values}\n\nfor i,query_vector in enumerate(tfidf_queries_matrix):\n    query_id = queries_df['query_id'].iloc[i]\n    relevant_docs = set(qdrel_df[qdrel_df['query_id'] == query_id]['doc_id'])\n\n    # evaluate cosine similarities\n    cosine_similarities = cosine_similarity(query_vector, tfidf_doc_matrix)[0]\n    sorted_indices = np.argsort(cosine_similarities)[::-1]\n\n    # for each k calculating Precision@k\n    for k in k_values:\n        top_k_indices =sorted_indices[:k]\n        retrieved_docs = set(docs_df['doc_id'].iloc[top_k_indices])\n\n        # Calculating Precision@k\n        precision = 1 if (len(relevant_docs.intersection(retrieved_docs)) / k) >0  else 0\n        precision_at_k[k].append(precision)\n\naverage_precision_at_k = {k: np.mean(scores) for k, scores in precision_at_k.items()}\n\n# print the results\nfor k, avg_precision in average_precision_at_k.items():\n    print(f\"\\nAverage Precision@{k}: {avg_precision}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:44:14.664713Z","iopub.execute_input":"2024-02-04T09:44:14.665681Z","iopub.status.idle":"2024-02-04T09:46:19.328505Z","shell.execute_reply.started":"2024-02-04T09:44:14.665632Z","shell.execute_reply":"2024-02-04T09:46:19.327285Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"\n Improve the performance of Task1 by lemmatization the tokens \n\nAfter lemmatization : \n\nThe vocabulary size :  (1585,)\n\n cosine similarity of tfidf_queries_matrix & tfidf_doc_matrix : \n [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n\nAverage Precision@1: 0.4\n\nAverage Precision@5: 0.58\n\nAverage Precision@10: 0.62\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 3","metadata":{}},{"cell_type":"markdown","source":"*   Improve the model from Task 2.2 further with Named Entity Recognition (NER) and Parts-Of-Speech (POS) tagging using spaCy.\n*   For each query and document vector, give more weightage to some important words. In essence, for each of the tf-idf vectors, multiply 2 along the dimensions which contain nouns, and multiply 4 for the named entities.\n*   Report the performance metrics ","metadata":{}},{"cell_type":"code","source":"print(\" \\n Improvement of the model from Task 2.2 further with Named Entity Recognition (NER) and Parts-Of-Speech(POS) tagging\\n \")\n# extract_E_P is a function to extract named entities and POS tags\ndef extract_E_P(text):\n    doc = nlp(text)\n    entities = [ent.text for ent in doc.ents]\n    pos_tags = [token.pos_ for token in doc]\n    return entities, pos_tags\n\n# apply it to document and queries\ndocs_df['entities'], docs_df['pos_tags'] = zip(*docs_df['doc_text_preprocessed'].apply(extract_E_P))\nqueries_df['entities'], queries_df['pos_tags'] = zip(*queries_df['query_text_corrected'].apply(extract_E_P))\n\n#tokenizing it \ntokenized_docs = []\nfor i in docs_df['doc_text_preprocessed']:\n    doc_tokens = [tok.text.lower() for tok in nlp(i) if tok.text.lower() in filtered_tokens]\n    tokenized_docs.append(doc_tokens)\n\ntokenized_queries = []\nfor query_text in queries_df['query_text_corrected']:\n    query_tokens = [tok.text.lower() for tok in nlp(query_text) if tok.text.lower() in filtered_tokens]\n    tokenized_queries.append(query_tokens)\n\nall_tokenized_texts = [\" \".join(tokens) for tokens in tokenized_docs + tokenized_queries]\n\n# creating if-idf vectorization\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(all_tokenized_texts)\n\ntfidf_doc_matrix = tfidf_matrix[:len(docs_df)]\ntfidf_queries_matrix = tfidf_matrix[len(docs_df):]\n\n# Append Named Entity Recognition (NER) and Parts-Of-Speech features to TF-IDF matrix\ntfidf_doc_matrix_with_features = pd.concat([pd.DataFrame(tfidf_doc_matrix.toarray()), docs_df[['entities', 'pos_tags']]], axis=1)\ntfidf_queries_matrix_with_features = pd.concat([pd.DataFrame(tfidf_queries_matrix.toarray()), queries_df[['entities', 'pos_tags']]], axis=1)\n\nk=vectorizer.get_feature_names_out()\nprint(\"\\n The vocabulary size : \",k.shape)\nx,y=extract_E_P(' '.join(k))\n\n# modify_tfidf_vector is a function that gives more weightage to \n# some important words - multiply 2 along the dimensions which contain nouns, and multiply 4 for the\n#  named entities.\ndef modify_tfidf_vector(tfidf_vector, pos_tags, entities):\n    if len(pos_tags) != tfidf_vector.shape[1]:\n        raise ValueError(\"Number of dimensions in TF-IDF matrix is not matching with length of pos_tags.\")\n\n    noun_indices = [i for i, pos_tag in enumerate(pos_tags) if 'NOUN' in pos_tag]\n\n    if max(noun_indices, default=-1) >= tfidf_vector.shape[1]:\n        raise ValueError(\"Index in noun_indices is out of range\")\n\n    tfidf_vector[:, noun_indices] *= 2\n    \n    entity_indices = [i for i,j in enumerate(k) if j in entities]\n\n    if max(entity_indices, default=-1) >= tfidf_vector.shape[1]:\n        raise ValueError(\"Index in entity_indices is out of range\")\n\n    tfidf_vector[:, entity_indices] *= 4\n\n    return tfidf_vector\n\n#do the modifications in doc and queries matrix\nmodified_tfidf_doc_matrix = modify_tfidf_vector(tfidf_doc_matrix, y,x)\nmodified_tfidf_queries_matrix = modify_tfidf_vector(tfidf_queries_matrix, y, x)\n\n# Calculating cosine similarity \ncosine_similarities = cosine_similarity(modified_tfidf_queries_matrix, modified_tfidf_doc_matrix)\n# print(cosine_similarities)\n\n# calculating precision\nprecision_at_k = {k: [] for k in k_values}\n\nfor i,query_vector in enumerate(tfidf_queries_matrix):\n    query_id = queries_df['query_id'].iloc[i]\n    relevant_docs = set(qdrel_df[qdrel_df['query_id'] == query_id]['doc_id'])\n\n    # evaluate cosine similarities\n    cosine_similarities = cosine_similarity(query_vector, tfidf_doc_matrix)[0]\n    sorted_indices = np.argsort(cosine_similarities)[::-1]\n\n    # for each k calculating Precision@k\n    for k in k_values:\n        top_k_indices =sorted_indices[:k]\n        retrieved_docs = set(docs_df['doc_id'].iloc[top_k_indices])\n\n        # Calculating Precision@k\n        precision = 1 if (len(relevant_docs.intersection(retrieved_docs)) / k) >0  else 0\n        precision_at_k[k].append(precision)\n\n# Calculating average Precision@k scores\naverage_precision_at_k = {k: np.mean(scores) for k, scores in precision_at_k.items()}\n\n# print result\nfor k, avg_precision in average_precision_at_k.items():\n    print(f\"\\nAverage Precision@{k}: {avg_precision}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:46:19.330253Z","iopub.execute_input":"2024-02-04T09:46:19.330664Z","iopub.status.idle":"2024-02-04T09:48:20.225315Z","shell.execute_reply.started":"2024-02-04T09:46:19.330628Z","shell.execute_reply":"2024-02-04T09:48:20.224171Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":" \n Improvement of the model from Task 2.2 further with Named Entity Recognition (NER) and Parts-Of-Speech(POS) tagging\n \n\n The vocabulary size :  (1585,)\n\nAverage Precision@1: 0.43\n\nAverage Precision@5: 0.56\n\nAverage Precision@10: 0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}